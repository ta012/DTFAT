Both absolute-path//pytorch_home/ and absolute-path/pytorch_home/ not present Using default
SET_PATH /vol/research/fmodel_av/tony/pytorch_home/
Setting TORCH_HOME to /vol/research/fmodel_av/tony/pytorch_home/
Setting HF_HOME to /vol/research/fmodel_av/tony/pytorch_home/
Setting PIP_CACHE_DIR to /vol/research/fmodel_av/tony/pytorch_home/
I am process 18543, running on cvplws262: starting (Sun Apr  7 06:55:58 2024)
---------------the evaluation dataloader---------------
now using following mask: 0 freq, 0 time
now using mix-up with rate 0.000000
now process audioset
use dataset mean -4.268 and std 4.569 to normalize the input.
number of classes is 527

val dataset size 18886 

in stem stride 1 same pad  1216
in stem stride 2 same pad  73792

dpr  [[0.0, 0.030000001192092896], [0.06000000238418579, 0.09000000357627869], [0.12000000476837158, 0.15000000596046448, 0.18000000715255737, 0.21000000834465027, 0.24000000953674316], [0.27000001072883606, 0.30000001192092896]]
window_size  (8, 8)
In stride 2 window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size  (8, 8)
In stride 2 window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size  (8, 8)
In stride 2 window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size_time,window_size_freq  (6, 3) (3, 6)
window_size  (8, 4)
In stride 2 window_size_time,window_size_freq  (4, 3) (3, 4)
window_size_time,window_size_freq  (4, 3) (3, 4)
 Loading ImgNet Pretrained weight 

 load_from  hf-hub timm/maxvit_small_tf_384.in1k

cached_file  /vol/research/fmodel_av/tony/pytorch_home/hub/models--timm--maxvit_small_tf_384.in1k/snapshots/1a3002966ada62604bbc6e0f6336b8eec288e3fa/pytorch_model.bin

 pop head.fc.weight	head.fc.bias
averaging input channel 3 kernel channel 1 

stem kernel channel 1 averaged 
Stem param stem.conv1.weight weight torch.Size([64, 1, 3, 3])
Updating after reshaping stem.conv1_t.weight torch.Size([64, 1, 6, 3])
Updating after reshaping stem.conv1_f.weight torch.Size([64, 1, 3, 6])
Stem param stem.conv1.bias weight torch.Size([64])
Updating without reshaping stem.conv1_t.bias torch.Size([64])
Updating without reshaping stem.conv1_f.bias torch.Size([64])
Stem param stem.norm1.weight weight torch.Size([64])
Updating without reshaping stem.norm1_t.weight torch.Size([64])
Updating without reshaping stem.norm1_f.weight torch.Size([64])
Stem param stem.norm1.bias weight torch.Size([64])
Updating without reshaping stem.norm1_t.bias torch.Size([64])
Updating without reshaping stem.norm1_f.bias torch.Size([64])
Stem param stem.norm1.running_mean weight torch.Size([64])
Updating without reshaping stem.norm1_t.running_mean torch.Size([64])
Updating without reshaping stem.norm1_f.running_mean torch.Size([64])
Stem param stem.norm1.running_var weight torch.Size([64])
Updating without reshaping stem.norm1_t.running_var torch.Size([64])
Updating without reshaping stem.norm1_f.running_var torch.Size([64])
Stem param stem.norm1.num_batches_tracked weight torch.Size([])
Updating without reshaping stem.norm1_t.num_batches_tracked torch.Size([])
Updating without reshaping stem.norm1_f.num_batches_tracked torch.Size([])
Stem param stem.conv2.weight weight torch.Size([64, 64, 3, 3])
Updating after reshaping stem.conv2_t.weight torch.Size([64, 64, 6, 3])
Updating after reshaping stem.conv2_f.weight torch.Size([64, 64, 3, 6])
Stem param stem.conv2.bias weight torch.Size([64])
Updating without reshaping stem.conv2_t.bias torch.Size([64])
Updating without reshaping stem.conv2_f.bias torch.Size([64])

Updating key ('stages.0.blocks.0.conv.conv2_kxk.weight', 'stages.0.blocks.0.conv.conv2_kxk_t.weight', 'stages.0.blocks.0.conv.conv2_kxk_f.weight')
Updating after reshaping stages.0.blocks.0.conv.conv2_kxk_t.weight torch.Size([384, 1, 6, 3])
Updating after reshaping stages.0.blocks.0.conv.conv2_kxk_f.weight torch.Size([384, 1, 3, 6])

Updating key ('stages.0.blocks.1.conv.conv2_kxk.weight', 'stages.0.blocks.1.conv.conv2_kxk_t.weight', 'stages.0.blocks.1.conv.conv2_kxk_f.weight')
Updating after reshaping stages.0.blocks.1.conv.conv2_kxk_t.weight torch.Size([384, 1, 6, 3])
Updating after reshaping stages.0.blocks.1.conv.conv2_kxk_f.weight torch.Size([384, 1, 3, 6])

Updating key ('stages.1.blocks.0.conv.conv2_kxk.weight', 'stages.1.blocks.0.conv.conv2_kxk_t.weight', 'stages.1.blocks.0.conv.conv2_kxk_f.weight')
Updating after reshaping stages.1.blocks.0.conv.conv2_kxk_t.weight torch.Size([768, 1, 6, 3])
Updating after reshaping stages.1.blocks.0.conv.conv2_kxk_f.weight torch.Size([768, 1, 3, 6])

Updating key ('stages.1.blocks.1.conv.conv2_kxk.weight', 'stages.1.blocks.1.conv.conv2_kxk_t.weight', 'stages.1.blocks.1.conv.conv2_kxk_f.weight')
Updating after reshaping stages.1.blocks.1.conv.conv2_kxk_t.weight torch.Size([768, 1, 6, 3])
Updating after reshaping stages.1.blocks.1.conv.conv2_kxk_f.weight torch.Size([768, 1, 3, 6])

Updating key ('stages.2.blocks.0.conv.conv2_kxk.weight', 'stages.2.blocks.0.conv.conv2_kxk_t.weight', 'stages.2.blocks.0.conv.conv2_kxk_f.weight')
Updating after reshaping stages.2.blocks.0.conv.conv2_kxk_t.weight torch.Size([1536, 1, 6, 3])
Updating after reshaping stages.2.blocks.0.conv.conv2_kxk_f.weight torch.Size([1536, 1, 3, 6])

Updating key ('stages.2.blocks.1.conv.conv2_kxk.weight', 'stages.2.blocks.1.conv.conv2_kxk_t.weight', 'stages.2.blocks.1.conv.conv2_kxk_f.weight')
Updating after reshaping stages.2.blocks.1.conv.conv2_kxk_t.weight torch.Size([1536, 1, 6, 3])
Updating after reshaping stages.2.blocks.1.conv.conv2_kxk_f.weight torch.Size([1536, 1, 3, 6])

Updating key ('stages.2.blocks.2.conv.conv2_kxk.weight', 'stages.2.blocks.2.conv.conv2_kxk_t.weight', 'stages.2.blocks.2.conv.conv2_kxk_f.weight')
Updating after reshaping stages.2.blocks.2.conv.conv2_kxk_t.weight torch.Size([1536, 1, 6, 3])
Updating after reshaping stages.2.blocks.2.conv.conv2_kxk_f.weight torch.Size([1536, 1, 3, 6])

Updating key ('stages.2.blocks.3.conv.conv2_kxk.weight', 'stages.2.blocks.3.conv.conv2_kxk_t.weight', 'stages.2.blocks.3.conv.conv2_kxk_f.weight')
Updating after reshaping stages.2.blocks.3.conv.conv2_kxk_t.weight torch.Size([1536, 1, 6, 3])
Updating after reshaping stages.2.blocks.3.conv.conv2_kxk_f.weight torch.Size([1536, 1, 3, 6])

Updating key ('stages.2.blocks.4.conv.conv2_kxk.weight', 'stages.2.blocks.4.conv.conv2_kxk_t.weight', 'stages.2.blocks.4.conv.conv2_kxk_f.weight')
Updating after reshaping stages.2.blocks.4.conv.conv2_kxk_t.weight torch.Size([1536, 1, 6, 3])
Updating after reshaping stages.2.blocks.4.conv.conv2_kxk_f.weight torch.Size([1536, 1, 3, 6])

Updating key ('stages.3.blocks.0.conv.conv2_kxk.weight', 'stages.3.blocks.0.conv.conv2_kxk_t.weight', 'stages.3.blocks.0.conv.conv2_kxk_f.weight')
Updating after reshaping stages.3.blocks.0.conv.conv2_kxk_t.weight torch.Size([3072, 1, 4, 3])
Updating after reshaping stages.3.blocks.0.conv.conv2_kxk_f.weight torch.Size([3072, 1, 3, 4])

Updating key ('stages.3.blocks.1.conv.conv2_kxk.weight', 'stages.3.blocks.1.conv.conv2_kxk_t.weight', 'stages.3.blocks.1.conv.conv2_kxk_f.weight')
Updating after reshaping stages.3.blocks.1.conv.conv2_kxk_t.weight torch.Size([3072, 1, 4, 3])
Updating after reshaping stages.3.blocks.1.conv.conv2_kxk_f.weight torch.Size([3072, 1, 3, 4])
init  stem.t_f_weight
init  stages.0.blocks.0.conv.t_f_weight
init  stages.0.blocks.1.conv.t_f_weight
init  stages.1.blocks.0.conv.t_f_weight
init  stages.1.blocks.1.conv.t_f_weight
init  stages.2.blocks.0.conv.t_f_weight
init  stages.2.blocks.1.conv.t_f_weight
init  stages.2.blocks.2.conv.t_f_weight
init  stages.2.blocks.3.conv.t_f_weight
init  stages.2.blocks.4.conv.t_f_weight
init  stages.3.blocks.0.conv.t_f_weight
init  stages.3.blocks.1.conv.t_f_weight
relative_pos_keys,  ['stages.0.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.0.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.0.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.0.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.1.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.1.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.1.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.1.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.2.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.2.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.3.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.3.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.4.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.2.blocks.4.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.3.blocks.0.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.3.blocks.0.attn_grid.attn.rel_pos.relative_position_bias_table', 'stages.3.blocks.1.attn_block.attn.rel_pos.relative_position_bias_table', 'stages.3.blocks.1.attn_grid.attn.rel_pos.relative_position_bias_table']

 All the keys in params in model.state_dict except relative pos keys head.fc are missing so good to load with strict = False

 Params not init after after_skip_formats []
timm forward_features out  torch.Size([1, 527])
count_parameters  69.039975

Loading ../../pretrained_models/audioset_fullset/best_model/models/best_audio_model.pth

 Now starting validating with best model 

Val loader size  591
0/591
1/591
2/591
3/591
4/591
5/591
6/591
7/591
8/591
9/591
10/591
11/591
12/591
13/591
14/591
15/591
16/591
17/591
18/591
19/591
20/591
21/591
22/591
23/591
24/591
25/591
26/591
27/591
28/591
29/591
30/591
31/591
32/591
33/591
34/591
35/591
36/591
37/591
38/591
39/591
40/591
41/591
42/591
43/591
44/591
45/591
46/591
47/591
48/591
